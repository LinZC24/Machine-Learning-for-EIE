{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F  #激活函数\n",
    "from torch import nn\n",
    "from torch import optim  #优化器\n",
    "from torchvision import transforms as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(root_dir):\n",
    "  paths=[]\n",
    "  labels=[]\n",
    "  for root, dirs, files in os.walk(root_dir):\n",
    "    for file in files:\n",
    "      paths.append(os.path.join(root, file))\n",
    "      index=file.find('_')\n",
    "      labels.append(file[:index])\n",
    "  return paths,labels\n",
    "\n",
    "def get_folder(root_dir):\n",
    "  folder_name=[]\n",
    "  for entry in os.listdir(root_dir):\n",
    "    full_path=os.path.join(root_dir, entry)\n",
    "    if os.path.isdir(full_path):\n",
    "      folder_name.append(entry)\n",
    "  return folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CifarDataset(torch.utils.data.Dataset):\n",
    "  def __init__(self, root_dir):\n",
    "    #初始化数据集，包含一系列图片与对应标签\n",
    "    super().__init__()\n",
    "    #self.data_path=root_dir\n",
    "    dataset=get_folder(root_dir)\n",
    "    data, labels=get_data(root_dir)\n",
    "    self.data=data\n",
    "    self.label=labels\n",
    "    if(dataset[0][-4:]=='test'):\n",
    "      #test_labels=get_folder(dataset[0])\n",
    "      test_data, test_labels=get_data(dataset[0])\n",
    "      #train_labels=get_folder(dataset[1])\n",
    "      train_data, train_labels=get_data(dataset[1])\n",
    "    else:\n",
    "      #test_labels=get_folder(dataset[1])\n",
    "      test_data, test_labels=get_data(dataset[1])\n",
    "      #train_labels=get_folder(dataset[0])\n",
    "      train_data, train_labels=get_data(dataset[0])\n",
    "    self.train_label=train_labels\n",
    "    self.train=train_data\n",
    "    self.test_label=test_labels\n",
    "    self.test=test_data\n",
    "    #raise NotImplementedError\n",
    "  \n",
    "  def __len__(self):\n",
    "    #返回数据集大小\n",
    "    return len(self.data)\n",
    "    #raise NotImplementedError\n",
    "  \n",
    "  def __getitem__(self, index):\n",
    "    #返回第index个样本\n",
    "    return self.data[index], self.label[index]\n",
    "    #raise NotImplementedError\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean(dataset):\n",
    "  expect_sum, var_sum=torch.zeros(3), torch.zeros(3)\n",
    "  size=len(dataset)\n",
    "  transform=tf.Compose([tf.ToTensor(), tf.Normalize((0,), (1,))])\n",
    "  for i in dataset:\n",
    "    image=Image.open(i[0])\n",
    "    img_tensor=transform(image)\n",
    "    expect_sum=expect_sum+torch.mean(img_tensor, dim=[1,2])\n",
    "    var_sum=var_sum+torch.mean(img_tensor**2, dim=[1,2])\n",
    "  mean=expect_sum/size\n",
    "  std=var_sum/size\n",
    "  return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FCNModel\n",
    "def get_dataloader(train):\n",
    "  dataset=CifarDataset('..\\cifar10')\n",
    "  mean, std=get_mean(dataset)\n",
    "  t=tf.Compose([tf.ToTensor(), tf.Normalize(mean, std)])\n",
    "  i, j=0, 0\n",
    "  for i in range(len(dataset.train)):\n",
    "    dataset.train[i]=t(Image.open(dataset.train[i]))\n",
    "  for j in range(len(dataset.test)):\n",
    "    dataset.test[j]=t(Image.open(dataset.test[j]))\n",
    "  if(train):\n",
    "    dataset=list(zip(dataset.train, dataset.train_label))\n",
    "  else:\n",
    "    dataset=list(zip(dataset.test, dataset.test_label))\n",
    "  \n",
    "  batch_size=train_batch_size if train else test_batch_size\n",
    "  dataloader=torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "  #print(dataset.data.size())\n",
    "  return dataloader\n",
    "\n",
    "train_batch_size=128\n",
    "test_batch_size=64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNModel(torch.nn.Module):\n",
    "  def __init__(self):\n",
    "    super(FCNModel, self).__init__()\n",
    "    self.fc1=torch.nn.Linear(32*32*1, 32)\n",
    "    self.fc2=(32, 10)\n",
    "\n",
    "  def forward(self, input_data):\n",
    "    x=input_data.view(-1, 28*28*1)\n",
    "    x=self.fc1(x)\n",
    "    x=F.relu(x)\n",
    "    x=self.fc2(x)\n",
    "    out=F.log_softmax(x, dim=-1)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcn_model=FCNModel()\n",
    "optimizer=torch.optim.Adam(fcn_model.parameters(), lr=0.001)\n",
    "train_loss_list=[]\n",
    "train_count_list=[]\n",
    "def train(epoch):\n",
    "  fcn_model.train(True)\n",
    "  train_dataloader=get_dataloader(train)\n",
    "  print(\"start training\")\n",
    "  for id, (train_data, train_label) in enumerate(train_dataloader):\n",
    "    optimizer.zero_grad()\n",
    "    out=fcn_model(train_data)\n",
    "    loss=F.nll_loss(out, train_label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if id%200==0:\n",
    "      print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch,  id * len(train_data), len(train_dataloader.dataset),100. * id / len(train_dataloader), loss.item()))\n",
    "      train_loss_list.append(loss.item())\n",
    "      train_count_list.append(id*train_batch_size+(epoch-1)*len(train_dataloader))\n",
    "  print('end training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "num_samples should be a positive integer value, but got num_samples=0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[119], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m      2\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m----> 3\u001b[0m     train(i)\n",
      "Cell \u001b[1;32mIn[118], line 7\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(epoch)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(epoch):\n\u001b[0;32m      6\u001b[0m   fcn_model\u001b[38;5;241m.\u001b[39mtrain(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m----> 7\u001b[0m   train_dataloader\u001b[38;5;241m=\u001b[39mget_dataloader(train)\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart training\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mid\u001b[39m, (train_data, train_label) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n",
      "Cell \u001b[1;32mIn[116], line 17\u001b[0m, in \u001b[0;36mget_dataloader\u001b[1;34m(train)\u001b[0m\n\u001b[0;32m     14\u001b[0m   dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(dataset\u001b[38;5;241m.\u001b[39mtest, dataset\u001b[38;5;241m.\u001b[39mtest_label))\n\u001b[0;32m     16\u001b[0m batch_size\u001b[38;5;241m=\u001b[39mtrain_batch_size \u001b[38;5;28;01mif\u001b[39;00m train \u001b[38;5;28;01melse\u001b[39;00m test_batch_size\n\u001b[1;32m---> 17\u001b[0m dataloader\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mDataLoader(dataset, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m#print(dataset.data.size())\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataloader\n",
      "File \u001b[1;32mf:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m RandomSampler(dataset, generator\u001b[38;5;241m=\u001b[39mgenerator)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mf:\\Anaconda3\\Lib\\site-packages\\torch\\utils\\data\\sampler.py:143\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: num_samples should be a positive integer value, but got num_samples=0"
     ]
    }
   ],
   "source": [
    "if __name__ =='__main__':\n",
    "  for i in range(5):\n",
    "    train(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存一下模型和优化器\n",
    "torch.save(fcn_model.state_dict(),\"fcn_model.pt\") #保存模型参数\n",
    "torch.save(optimizer.state_dict(), 'optimizer.pt') #保存优化器参数\n",
    "# state_dict变量存放训练过程中需要学习的权重，state_dict作为python的字典对象将每一层的参数映射成tensor张量\n",
    "\n",
    "# 模型的加载\n",
    "fcn_model.load_state_dict(torch.load(\"fcn_model.pt\"))\n",
    "optimizer.load_state_dict(torch.load(\"optimizer.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    fcn_model.eval()  #设置为评估模式\n",
    "    test_dataloader = get_dataloader(train=False)  #导入测试数据集\n",
    "    with torch.no_grad():  #不需要计算梯度\n",
    "        for data, label in test_dataloader:\n",
    "            output = fcn_model(data)\n",
    "            test_loss += F.nll_loss(output, label, reduction='sum').item()  \n",
    "            pred = output.data.max(1, keepdim=True)[1] #获取最大值的位置\n",
    "            # 第一个1表示找第2维的最大值（如果是第1维的最大值则设为0）\n",
    "            # output.data.max(2, keepdim=True)会返回一个数组，第一个是output数组中第2维度的最大值是多少，\n",
    "            # 第二个是最大值的位置在哪里\n",
    "            # 因此[1]表示此时只关心最大值是多少\n",
    "            correct += pred.eq(label.data.view_as(pred)).sum()\n",
    "    test_loss /= len(test_dataloader.dataset)  #计算平均损失\n",
    "    print('\\nTest set: Avg. loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_dataloader.dataset),\n",
    "        100. * correct / len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CNNModel\n",
    "def get_dataloader(train):\n",
    "  dataset=CifarDataset('..\\cifar10')\n",
    "  mean, std=get_mean(dataset)\n",
    "  dataset=tf.Compose([tf.ToTensor(), tf.Normalize(mean, std)])\n",
    "  batch_size=train_batch_size if train else test_batch_size\n",
    "  dataloader=torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "  return dataloader\n",
    "\n",
    "class CNNModel(torch.nn.Module):\n",
    "  def __init__(self, *args, **kwargs) -> None:\n",
    "    super().__init__(*args, **kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
